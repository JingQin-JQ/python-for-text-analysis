{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: FUNCTIONS AND FILES\n",
    "\n",
    "# Due: Friday the 24th of November 2017 23:59 p.m.\n",
    "\n",
    "*Please name your ipython notebook with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.ipynb*\n",
    "\n",
    "*Please submit your assignment using [this google form](https://docs.google.com/forms/d/e/1FAIpQLSf_LKXRj-4EBre49mrN1rIKzrMMZ6PrG9SFmEDfvIbUH3EeJA/viewform)*\n",
    "\n",
    "*If you have questions about this topic, please send an email to f.ilievski@vu.nl*\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "*Hint \\#1 for the overall assignment: Use docstrings to describe your functions.*\n",
    "\n",
    "\n",
    "## Functions, containers, and edge cases \n",
    "\n",
    "** Exercise 1 **\n",
    "\n",
    "Write a function that checks if a parameter `character` is a vowel. The value of this variable should be supplied by the user of the program. Make sure that your function handles the following \"edge cases\" (edge cases are a common term to refer to unexpected/extreme values of parameters, you can read more [here](https://en.wikipedia.org/wiki/Edge_case)) :\n",
    "* **1a** if there is more than one character in the input, then print whether the last character is a vowel (bonus reward if you manage to do this by recursively calling the function from inside itself)\n",
    "* **1b** if there is zero characters in the input, print an informative message and instruct the user to enter exactly one letter character\n",
    "* **1c** if the character is not a letter, then print an informative message and instruct the user to enter one letter character \n",
    "\n",
    "*Hint \\#1: For the last two points, define and use a separate function called `print_info()` to print the instructions.*\n",
    "\n",
    "*Hint \\#2: Define the vowels yourself as a container.*\n",
    "\n",
    "*Hint \\#3: there is a method of the string class that tells you whether something is a letter. You can use this one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Write a generic text processing function that does various tasks on top of a textual input, which is supplied as a positional argument to this function. What exactly is done with this `text` depends on the value of the second positional parameter `task`:\n",
    "* **2a** If `task` has value \"T\", then the function should return all tokens, ordered by their appearance. Add a keyword argument `min_size` to the text processing function, with a default value of 1 that will make the function only return the tokens that are longer than this `min_size`. \n",
    "* **2b** If `task` has value \"U\", then the function should return all unique tokens found in text, not necessarily in any specific order. Use another keyword argument `tokens_to_skip` that will contain all tokens that should not be included in the final set. The default value of this variable contains the tokens \"a\", \"an\" and \"the\".\n",
    "* **2c** If `task` has value \"C\", then return a data structure that counts the number of occurences of each of the vowels in that text.\n",
    "* **2d** If `task` has some other value than the three listed above, print a friendly instruction message together with the first 10 characters of the supplied text.\n",
    "\n",
    "*Hint \\#1: Use the right data structure for each of the subtasks.*\n",
    "\n",
    "*Hint \\#2: You are allowed to use 'helper' functions if part of the code is repeated across the exercises 2a, 2b, and 2c. While this is not strictly required, it will make your life easier.*\n",
    "\n",
    "*Hint \\#3: Test your function! Try different texts, tasks and arguments until you are fairly confident that it works as expected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2e** Explain briefly the motivation behind the data structures (containers) you chose in each of the three sub-exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table structures\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Write a function that reads a TSV/CSV file, then swaps the values of the first two columns, and writes it back to disk as TSV/CSV file. \n",
    "* **3a** The name of the CSV/TSV file is a compulsory parameter to this function.\n",
    "* **3b** The formats of the input and the output files are also given as parameters to the function. The default for the input format is 'tsv', the default for the output format is 'csv'.\n",
    "* **3c** The function has another parameter `header` which contains a boolean value and tells the function if there is a header row in the input file or not. If there is a header row, your function should add an asterisk (\\*) in front of all columns of the header. \n",
    "* **3d** If the input file has less than two columns, you can not perform the switch. There are two subcases here: If there are no columns at all (empty file), then inform the user that the file is empty. If there is only one column, generate a random number in a second column (for all rows). Make sure that you also add a header column \"number\" for this column 2 if the `header` argument is `True`.\n",
    "\n",
    "For example, let's say the input format is CSV, the output is CSV and the header value is True. And we get a file that contains this input table:\n",
    "\n",
    "`name\n",
    "Filip\n",
    "Marten\n",
    "Chantal\n",
    "...`\n",
    "\n",
    "Then we have to return the following:\n",
    "\n",
    "`*name,*number\n",
    "Filip,2\n",
    "Marten,18\n",
    "Chantal,43 \n",
    "...`\n",
    "\n",
    "*Hint \\#1: You can use the CSV file in \"../Data/baby_names/names_by_state/AK.TXT\" to test this function with CSV as an input.* \n",
    "\n",
    "*Hint \\#2: Once your function works well, you can help yourself: by using the function, you can create a TSV file which you can then also use as an input. Alternatively, you can test the function with the file \"../Data/baby_names/names_by_state/AK.tsv\". You can create simple \"dummy\" test files also outside of Python (for example, with Excel) - but in this case make sure that you store it as CSV and in the write path.*\n",
    "\n",
    "*Hint \\#3: Make sure you test your function for the edge case in 3d. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with the NLTK package\n",
    "\n",
    "Sometimes authors in NLP implement their findings in the NLTK, so as to make their software available to everyone. This happened with the VADER-tool (Valence  Aware  Dictionary  for sEntiment Reasoning, [Hutto & Gilbert 2014](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf), full code [here](http://www.nltk.org/_modules/nltk/sentiment/vader.html)). As the name suggests, you can use this tool to determine the sentiment of a text. Reading the paper, you can see the algorithm itself is really simple, but it manages to capture sentiment pretty well. (And yes, we know the acronym is horrible.)\n",
    "\n",
    "In this assignment, we'll put this tool to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the sentiment analyzer class.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you experience any error with the last cell, ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And instantiate a sentiment analyzer object.\n",
    "# NOTE: this will produce a warning, but you can safely ignore it.\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do you use this tool? Basically, you can just call it with any sentence you like! Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\"I am sad.\",\n",
    "             \"I am quite happy.\"]\n",
    "\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "* **4a** Of course this is a really simple example. You can probably think of some more challenging phrases to enter. Before we use the sentiment analyzer to answer some research questions, let's first get some intuitions about what the tool does well, and where it makes mistakes. Try to come up with **three** challenging examples (hint: statements with a non-literal meaning) for the tool to judge, and write down your findings as a comment.\n",
    "\n",
    "*Hint \\#1: This is just to get a sense of what the tool does. Don't spend too much time on this exercise!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **4b** In the folder \"../Data/LCohen/\" you can find 10 text files from [this item](https://news.ycombinator.com/item?id=12926684), that has comments on a story in rollingstone.com about the death of Leonard Cohen. Make sure that the folder LCohen exists in your Data and that it contains 10 text files. If this is not the case, go to github and download them first. Use `glob` to list all \".txt\" files in this directory, then open each of these files for reading.\n",
    "\n",
    "* **4c** Then, use the VADER sentiment analysis tool to answer the following two questions:\n",
    "\n",
    " 1. What is the most positive comment in the Hackernews story about Leonard Cohen?\n",
    "\n",
    " 2. What is the most negative comment in the Hackernews story about Leonard Cohen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can copy the file name of the most positive comment here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can copy the file name of the most negative comment here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Getting the list of top-N (instead of top-1) comments that are positive/negative is much more challenging and requires additional lists and dictionaries to store the data and order it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "There is a lot of interesting data online. For instance, the [Hackernews API](https://github.com/HackerNews/API) is a nice source of discussions between people, and of opiniated text. We are going to retrieve some posts of:\n",
    "* Stephen Merity: a machine learning researcher who also has a nice tech blog ([JSON link here](https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty))\n",
    "* Gabriel Weinberg: founder of the DuckDuckGo search engine ([JSON link here](https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty))\n",
    "\n",
    "We expect that Stephen will talk more about technology, and Gabriel will talk more about the Web.\n",
    "\n",
    "Since we did not teach you how to download data, we are providing you with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty\"\n",
    "r_user=requests.get(download_user_uri)\n",
    "user_data = r_user.json()\n",
    "#print(user_data)\n",
    "items = user_data['submitted']\n",
    "for item in items[:2]: # download the first two items\n",
    "    download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "    r_text = requests.get(download_item_uri)\n",
    "    item_data = r_text.json()\n",
    "    content=item_data['text']\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above works and gives us two articles written by the author \"Smerity\". Good! But if we want to download, let's say, five articles written by another user now, we would have to copy and edit the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty\"\n",
    "r_user=requests.get(download_user_uri)\n",
    "user_data = r_user.json()\n",
    "#print(user_data)\n",
    "items = user_data['submitted']\n",
    "for item in items[:5]: # download the first five items\n",
    "    download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "    r_text = requests.get(download_item_uri)\n",
    "    item_data = r_text.json()\n",
    "    content=item_data['text']\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "We can see that a lot of the code is now repeated between the two code blocks. This is called \"spaghetti code\" and it is a very bad practice. Luckily, we learned about functions and we can apply them to *refactor* the code.\n",
    "\n",
    "* **5a** Create a function `download_user(user, max_items)` that will allow us to download the posts by any user just by supplying its username to this function. The parameter `max_items` lets us choose how many articles to download from each user.** \n",
    "* **5b** Use these two parameters inside the function to replace the fixed values of \"Smerity\" (for user) and 2 (for maximum items). The parameter `user` should be a positional parameter, while `max_items` should be a keyword parameter with a default value of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_user(user, max_items):\n",
    "    # Your code here\n",
    "    # don't forget to replace the mentions of the user and maximum items with the parameters of the function to make it general\n",
    "    \n",
    "    print(content)\n",
    "    \n",
    "# Now let's test this function\n",
    "merity = download_user('Smerity', 2)\n",
    "weinberg = download_user('epi0Bauqu', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** \n",
    "\n",
    "Now that we have removed some unnecesary repetition and gained flexibility, we could do another cycle of refactoring (note that for instance the 3 lines used to download user and item info are also similar). But instead, let's move on and prepare this data for linguistic processing:\n",
    "\n",
    "\n",
    "* **7a** create a list within the function called `contents`. Put every text article in this list and store the articles in a JSON in the end of the function. This JSON should look as follows:\n",
    "`{\"Smerity\":[\"this is the text of article 1\", \"this is the text of article 2\", ..., \"this is the text of article 10\"]}`.\n",
    "And similarly for the other user.\n",
    "* **7b** Now call this function and retrieve 10 texts per user.\n",
    "* **7c** Store the JSON dictionaries for both users in two separate JSON files \"../Data/Hackernews/cache_Smerity.json\" and \"../Data/Hackernews/cache_epi0Bauqu.json\". This is useful because your analysis will be replicable (maybe a user will delete their posts, or start posting about something completely different), and you won't have to download the data again. If this directory does not exist, create it.\n",
    "\n",
    "*Hint \\#1: Use the `os` module to create a directory if it does not exist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User profiling\n",
    "\n",
    "By downloading 10 articles from each user, we have practically created our first NLP corpus. We don't need to download *all* of their posts, a sample should be enough.\n",
    "\n",
    "**NOTE:** In case you had problems with downloading and storing the JSON files, you can download the two files directly from the `Data/Hackernews` folder on github.\n",
    "\n",
    "Let's see if we can do some user profiling, to judge what topics people like to talk about. As we said before, we expect that Stephen will talk more about technology, and Gabriel will talk more about the Web.\n",
    "\n",
    "We now need to **process the corpus** such that we can actually make a nice comparison between the two users. We will:\n",
    "\n",
    "* tokenize and lemmatize the posts by Merity and Weinberg.\n",
    "* Make two lists: `merity_lemmas` and `weinberg_lemmas`.\n",
    "* Use sets to determine the words *exclusively* used by Merity and Weinberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7** \n",
    "\n",
    "* **7a** Create a function that takes a string (the text of a comment), and returns a list of lemmas. Don't forget to use a docstring to say what the function does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_lemmas(text):\n",
    "    # Don't forget to write a docstring!\n",
    "    # YOUR CODE HERE.\n",
    "    \n",
    "    return list_of_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get this function (that generates lemmas for each post) for free :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_lemma_list(posts):\n",
    "    \"\"\"Function to generate a lemma list using a list of posts.\"\"\"\n",
    "    # This is the list that the function will use to collect all the lemmas.\n",
    "    lemma_list = []\n",
    "    for post in posts:\n",
    "    # First get the post\n",
    "        # Get the text from the post:\n",
    "        if 'text' in post:\n",
    "            text = post[\"text\"]\n",
    "        # Get the lemmas for the current text.\n",
    "        lemmas = text_to_lemmas(text)\n",
    "        # Extend the list with the lemmas found for the current text.\n",
    "        lemma_list.extend(lemmas)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7b** Now let's use JSON to load both user files. Try to put this also in a function, which returns a list of post texts. After that, we can generate lemma lists for both users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_user_from_json(user):\n",
    "    # function definition here\n",
    "    \n",
    "    return posts\n",
    "    \n",
    "merity_posts=load_user_from_json(\"Smerity\")\n",
    "weinberg_posts=load_user_from_json(\"epi0Bauqu\")\n",
    "\n",
    "# Get all the lemmas \n",
    "merity_lemmas = generate_lemma_list(merity_posts)\n",
    "weinberg_lemmas = generate_lemma_list(weinberg_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some set operations, it's very easy to find words that are only used by one person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute this cell.\n",
    "\n",
    "# Create sets of lemmas.\n",
    "merity_set = set(merity_lemmas)\n",
    "weinberg_set = set(weinberg_lemmas) \n",
    "\n",
    "# See which words are unique to each author.\n",
    "only_merity = merity_set - weinberg_set\n",
    "only_weinberg = weinberg_set - merity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can play around with the data in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7c** But more interesting are those words that are used by both authors. Use a similar set operation to see how many words are commonly used by both authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we find out which word is more typical for which author? We could easily implement log likelihood function based on [this page](http://ucrel.lancs.ac.uk/llwizard.html) by Paul Rayson. \n",
    "Log likelihood is a measure of 'unexpectedness' of a word in a particular corpus. We can compute it by comparing two corpora. We expect any given word to occur about the same amount of times in both corpora. If a word occurs in one corpus much more than in some other corpus, we get a high log likelihood score. We won't require this in the assignment, but if you have time, feel free to try and implement this too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Questions about Python\n",
    "\n",
    "* **8a** what is the difference between parameters and arguments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8b** can you give an example code that shows the local and global scope of a variable? Also add explanation as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8c** What is a difference between CSV and TSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8d** When would you use JSON and when would you choose a table (CSV/TSV)? Give an example of both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8e** How many for-loops can you nest in one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8f** Is it better to use file.close() or to use the contextmanager with the 'with' keyword? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8g** What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

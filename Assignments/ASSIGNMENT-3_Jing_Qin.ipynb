{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: FUNCTIONS AND FILES\n",
    "\n",
    "# Due: Friday the 24th of November 2017 23:59 p.m.\n",
    "\n",
    "*Please name your ipython notebook with the following naming convention: ASSIGNMENT_3_FIRSTNAME_LASTNAME.ipynb*\n",
    "\n",
    "*Please submit your assignment using [this google form](https://docs.google.com/forms/d/e/1FAIpQLSf_LKXRj-4EBre49mrN1rIKzrMMZ6PrG9SFmEDfvIbUH3EeJA/viewform)*\n",
    "\n",
    "*If you have questions about this topic, please send an email to f.ilievski@vu.nl*\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "*Hint \\#1 for the overall assignment: Use docstrings to describe your functions.*\n",
    "\n",
    "\n",
    "## Functions, containers, and edge cases \n",
    "\n",
    "** Exercise 1 **\n",
    "\n",
    "Write a function that checks if a parameter `character` is a vowel. The value of this variable should be supplied by the user of the program. Make sure that your function handles the following \"edge cases\" (edge cases are a common term to refer to unexpected/extreme values of parameters, you can read more [here](https://en.wikipedia.org/wiki/Edge_case)) :\n",
    "* **1a** if there is more than one character in the input, then print whether the last character is a vowel (bonus reward if you manage to do this by recursively calling the function from inside itself)\n",
    "* **1b** if there is zero characters in the input, print an informative message and instruct the user to enter exactly one letter character\n",
    "* **1c** if the character is not a letter, then print an informative message and instruct the user to enter one letter character \n",
    "\n",
    "*Hint \\#1: For the last two points, define and use a separate function called `print_info()` to print the instructions.*\n",
    "\n",
    "*Hint \\#2: Define the vowels yourself as a container.*\n",
    "\n",
    "*Hint \\#3: there is a method of the string class that tells you whether something is a letter. You can use this one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter one letter character: 21\n",
      "Please enter one letter character: 1\n",
      "Please enter one letter character: \n",
      "Please enter exactly one letter character: d\n",
      "The character is not a vowel\n"
     ]
    }
   ],
   "source": [
    "def check_vowel(character):\n",
    "    vowels = [\"a\", \"o\", \"e\", \"i\", \"u\"]\n",
    "    if len(character) > 1:\n",
    "        print(\"There are more than one characters, we will only consider the last character.\")\n",
    "        check_vowel(character[-1])\n",
    "    elif character.lower() in vowels:\n",
    "        print(\"The character is a vowel\")\n",
    "    else:\n",
    "        print(\"The character is not a vowel\")\n",
    "    return\n",
    "\n",
    "def print_info(character):\n",
    "    if len(character) == 0:\n",
    "        return \"Please enter exactly one letter character: \"\n",
    "    else:\n",
    "        return \"Please enter one letter character: \"\n",
    "    \n",
    "\n",
    "character = input(\"Please enter one letter character: \")\n",
    "while len(character) == 0 or not character.isalpha():\n",
    "            character =input(print_info(character))        \n",
    "check_vowel(character)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Write a generic text processing function that does various tasks on top of a textual input, which is supplied as a positional argument to this function. What exactly is done with this `text` depends on the value of the second positional parameter `task`:\n",
    "* **2a** If `task` has value \"T\", then the function should return all tokens, ordered by their appearance. Add a keyword argument `min_size` to the text processing function, with a default value of 1 that will make the function only return the tokens that are longer than this `min_size`. \n",
    "* **2b** If `task` has value \"U\", then the function should return all unique tokens found in text, not necessarily in any specific order. Use another keyword argument `tokens_to_skip` that will contain all tokens that should not be included in the final set. The default value of this variable contains the tokens \"a\", \"an\" and \"the\".\n",
    "* **2c** If `task` has value \"C\", then return a data structure that counts the number of occurences of each of the vowels in that text.\n",
    "* **2d** If `task` has some other value than the three listed above, print a friendly instruction message together with the first 10 characters of the supplied text.\n",
    "\n",
    "*Hint \\#1: Use the right data structure for each of the subtasks.*\n",
    "\n",
    "*Hint \\#2: You are allowed to use 'helper' functions if part of the code is repeated across the exercises 2a, 2b, and 2c. While this is not strictly required, it will make your life easier.*\n",
    "\n",
    "*Hint \\#3: Test your function! Try different texts, tasks and arguments until you are fairly confident that it works as expected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input some text: 2a If task has value \"T\", then the function should return all tokens, ordered by their appearance. Add a keyword argument min_size to the text processing function, with a default value of 1 that will make the function only return the tokens that are longer than this min_size. 2b If task has value \"U\", then the function should return all unique tokens found in text, not necessarily in any specific order. Use another keyword argument tokens_to_skip that will contain all tokens that should not be included in the final set. The default value of this variable contains the tokens \"a\", \"an\" and \"the\". 2c If task has value \"C\", then return a data structure that counts the number of occurences of each of the vowels in that text. 2d If task has some other value than the three listed above, print a friendly instruction message together with the first 10 characters of the supplied text.\n",
      "please input a task(T, U or C):C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 58, 'e': 84, 'i': 41, 'o': 42, 'u': 34}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "import nltk\n",
    "\n",
    "def process(text,task, min_size=1, tokens_to_skip = [\"a\",\"an\",\"the\"]):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if task == \"T\":\n",
    "        return [token for token in tokens if len(token) <= min_size]\n",
    "    if task == \"U\":\n",
    "        return set([token for token in tokens if token.lower() not in tokens_to_skip ])\n",
    "    if task == \"C\":\n",
    "        return {v:text.lower().count(v) for v in 'aeiou'}\n",
    "    else:\n",
    "        print(\"Please input the task(T, U or C) for the text:\" + text[:10])\n",
    "        \n",
    "\n",
    "text = input(\"please input some text: \")\n",
    "task = input(\"please input a task(T, U or C):\")\n",
    "process(text,task,min_size=1,tokens_to_skip = [\"a\",\"an\",\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **2e** Explain briefly the motivation behind the data structures (containers) you chose in each of the three sub-exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2a returns a list of tokens, which ordered by their appearance.*\n",
    "\n",
    "*2b returns a set of tokens, which contain unique tokens without any specific order.*\n",
    "\n",
    "*2c returns a dictionary, have vowels as keys, their number of occurences as values.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table structures\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Write a function that reads a TSV/CSV file, then swaps the values of the first two columns, and writes it back to disk as TSV/CSV file. \n",
    "* **3a** The name of the CSV/TSV file is a compulsory parameter to this function.\n",
    "* **3b** The formats of the input and the output files are also given as parameters to the function. The default for the input format is 'tsv', the default for the output format is 'csv'.\n",
    "* **3c** The function has another parameter `header` which contains a boolean value and tells the function if there is a header row in the input file or not. If there is a header row, your function should add an asterisk (\\*) in front of all columns of the header. \n",
    "* **3d** If the input file has less than two columns, you can not perform the switch. There are two subcases here: If there are no columns at all (empty file), then inform the user that the file is empty. If there is only one column, generate a random number in a second column (for all rows). Make sure that you also add a header column \"number\" for this column 2 if the `header` argument is `True`.\n",
    "\n",
    "For example, let's say the input format is CSV, the output is CSV and the header value is True. And we get a file that contains this input table:\n",
    "\n",
    "`name\n",
    "Filip\n",
    "Marten\n",
    "Chantal\n",
    "...`\n",
    "\n",
    "Then we have to return the following:\n",
    "\n",
    "`*name,*number\n",
    "Filip,2\n",
    "Marten,18\n",
    "Chantal,43 \n",
    "...`\n",
    "\n",
    "*Hint \\#1: You can use the CSV file in \"../Data/baby_names/names_by_state/AK.TXT\" to test this function with CSV as an input.* \n",
    "\n",
    "*Hint \\#2: Once your function works well, you can help yourself: by using the function, you can create a TSV file which you can then also use as an input. Alternatively, you can test the function with the file \"../Data/baby_names/names_by_state/AK.tsv\". You can create simple \"dummy\" test files also outside of Python (for example, with Excel) - but in this case make sure that you store it as CSV and in the write path.*\n",
    "\n",
    "*Hint \\#3: Make sure you test your function for the edge case in 3d. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please  give an inputfile (csv or tsv file): ../Data/baby_names/names_by_state/dummy.tsv\n",
      "Please  give an outputfile (csv or tsv file): ../Data/baby_names/names_by_state/dummy.csv\n",
      "Is the file have a header(T or F):F\n",
      "['******e,******d\\n', 'g,s\\n', 'e,c\\n', 'fj,23\\n', '23,d\\n']\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import sys\n",
    "\n",
    "def swap(input_file,output_file,header_bool, input_format =\"csv\" , output_format = \"tsv\"):\n",
    "    with open(input_file, \"r\") as csvfile:\n",
    "        csv_data = []\n",
    "        \n",
    "        # Decide on the split sign by the types of input and output files\n",
    "        \n",
    "        if input_format == \"tsv\":\n",
    "            input_sign = \"\\t\"\n",
    "        elif input_format == \"csv\":\n",
    "            input_sign = \",\"\n",
    "        else:\n",
    "            return \" Wrong input_file format\"\n",
    "        \n",
    "        if output_format == \"tsv\":\n",
    "            output_sign = \"\\t\"\n",
    "        elif output_format == \"csv\":\n",
    "            output_sign = \",\"\n",
    "        else:\n",
    "            return \" Wrong output_file format\"\n",
    "        \n",
    "        content = csvfile.readlines()\n",
    "        \n",
    "        # Inform mpty file\n",
    "        \n",
    "        if len(content)== 0:\n",
    "                print(\"The file is empty!\")\n",
    "                return\n",
    "        \n",
    "        # Add \"*\" to header\n",
    "        \n",
    "        if header_bool:\n",
    "            headers = content[0].strip(\"\\n\").split(input_sign)\n",
    "            if len(headers) == 1:  # Add a new column header with \"*\"\n",
    "                headers.append(\"number\")\n",
    "            headers = [\"*\" + header for header in headers]\n",
    "            content[0] = input_sign.join(headers)   \n",
    "        \n",
    "        # Swap firset two columns\n",
    "        \n",
    "        for row in content:\n",
    "            row = row.strip(\"\\n\")  # remove all newlines\n",
    "            columns = row.split(input_sign)\n",
    "            if len(columns)> 1: # More than 1 column, swap directly\n",
    "                a = columns[0]\n",
    "                columns[0] = columns[1]\n",
    "                columns[1] = a\n",
    "            else:  # Add a new column with random int(0-1000) and swap\n",
    "                a = columns[0] \n",
    "                columns[0] = str(randint(0, 1000))\n",
    "                columns.append(a)\n",
    "            csv_data.append(columns)\n",
    "    \n",
    "    # Write to output file\n",
    "    \n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        for row in csv_data:\n",
    "            line = output_sign.join(row)+\"\\n\"\n",
    "            outfile.write(line)\n",
    "    \n",
    "    # Print output file for check\n",
    "    \n",
    "    with open(output_file, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        print (content)\n",
    "        \n",
    "in_file = input(\"Please  give an inputfile (csv or tsv file): \")\n",
    "out_file = input(\"Please  give an outputfile (csv or tsv file): \")\n",
    "h = input(\"Is the file have a header(T or F):\")\n",
    "if h == \"t\" or h ==\"T\":\n",
    "    header_bool = True,\n",
    "elif h ==\"f\" or h == \"F\":\n",
    "    header_bool = False\n",
    "else:\n",
    "    print (\"Wrong input!\")\n",
    "    sys.exit(0)\n",
    "in_format = in_file.split(\".\")[-1] \n",
    "out_format = out_file.split(\".\")[-1] \n",
    "swap(in_file,out_file,header_bool,input_format = in_format , output_format = out_format)\n",
    "\n",
    "# you can try for this filename = \"../Data/baby_names/names_by_state/dummy.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with the NLTK package\n",
    "\n",
    "Sometimes authors in NLP implement their findings in the NLTK, so as to make their software available to everyone. This happened with the VADER-tool (Valence  Aware  Dictionary  for sEntiment Reasoning, [Hutto & Gilbert 2014](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf), full code [here](http://www.nltk.org/_modules/nltk/sentiment/vader.html)). As the name suggests, you can use this tool to determine the sentiment of a text. Reading the paper, you can see the algorithm itself is really simple, but it manages to capture sentiment pretty well. (And yes, we know the acronym is horrible.)\n",
    "\n",
    "In this assignment, we'll put this tool to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the sentiment analyzer class.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you experience any error with the last cell, ask for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And instantiate a sentiment analyzer object.\n",
    "# NOTE: this will produce a warning, but you can safely ignore it.\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do you use this tool? Basically, you can just call it with any sentence you like! Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "I am sad.\n",
      "{'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n",
      "--------\n",
      "I am quite happy.\n",
      "{'neg': 0.0, 'neu': 0.334, 'pos': 0.666, 'compound': 0.6115}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I am sad.\",\n",
    "             \"I am quite happy.\"]\n",
    "\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "* **4a** Of course this is a really simple example. You can probably think of some more challenging phrases to enter. Before we use the sentiment analyzer to answer some research questions, let's first get some intuitions about what the tool does well, and where it makes mistakes. Try to come up with **three** challenging examples (hint: statements with a non-literal meaning) for the tool to judge, and write down your findings as a comment.\n",
    "\n",
    "*Hint \\#1: This is just to get a sense of what the tool does. Don't spend too much time on this exercise!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "One mile away, the hulking albino named Silas limped through the front gate of the luxurious brownstone residence on Rue La Bruyère.\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "--------\n",
      " The spiked cilice belt that he wore around his thigh cut into his flesh, and yet his soul sang with satisfaction of service to the Lord.\n",
      "{'neg': 0.072, 'neu': 0.828, 'pos': 0.1, 'compound': 0.2023}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"One mile away, the hulking albino named Silas limped through the front gate of the luxurious brownstone residence on Rue La Bruyère.\", \" The spiked cilice belt that he wore around his thigh cut into his flesh, and yet his soul sang with satisfaction of service to the Lord.\"]\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The first sentence seems neutral, the analysis works fine. The second sentence is hard to say it positively, and it also gives a bit weird feeling, the analysis can't get that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "In the passenger seat, Robert Langdon felt the city tear past him as he tried to clear his thoughts.\n",
      "{'neg': 0.0, 'neu': 0.874, 'pos': 0.126, 'compound': 0.3818}\n",
      "--------\n",
      "His quick shower and shave had left him looking reasonably presentable but had done little to ease his anxiety.\n",
      "{'neg': 0.094, 'neu': 0.777, 'pos': 0.129, 'compound': 0.198}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"In the passenger seat, Robert Langdon felt the city tear past him as he tried to clear his thoughts.\", \"His quick shower and shave had left him looking reasonably presentable but had done little to ease his anxiety.\"]\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Even in the sentence, there are no negative words, but we get feelings from \"tear\", \"left\",\"anxiety\", where the analysis can't get.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "The frightening image of the curator's body remained locked in his mind.\n",
      "{'neg': 0.225, 'neu': 0.775, 'pos': 0.0, 'compound': -0.4939}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"The frightening image of the curator's body remained locked in his mind.\"]\n",
    "print('--------')\n",
    "for sentence in sentences:\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    print(sentence)\n",
    "    print(scores)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis works great on this one, because of \"frightening\" is quite an obvious word.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **4b** In the folder \"../Data/LCohen/\" you can find 10 text files from [this item](https://news.ycombinator.com/item?id=12926684), that has comments on a story in rollingstone.com about the death of Leonard Cohen. Make sure that the folder LCohen exists in your Data and that it contains 10 text files. If this is not the case, go to github and download them first. Use `glob` to list all \".txt\" files in this directory, then open each of these files for reading.\n",
    "\n",
    "* **4c** Then, use the VADER sentiment analysis tool to answer the following two questions:\n",
    "\n",
    " 1. What is the most positive comment in the Hackernews story about Leonard Cohen?\n",
    "\n",
    " 2. What is the most negative comment in the Hackernews story about Leonard Cohen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "This is a wonderful article about Leonard Cohen:<p><a href=\"http:&#x2F;&#x2F;www.tabletmag.com&#x2F;jewish-arts-and-culture&#x2F;music&#x2F;89715&#x2F;leonard\" rel=\"nofollow\">http:&#x2F;&#x2F;www.tabletmag.com&#x2F;jewish-arts-and-culture&#x2F;music&#x2F;89715...</a><p>The quote that stood out the most for me:<p>&quot;In a pursuit like rock ’n’ roll, which is entirely devoted to redemption, Cohen’s ideas were not only old but radical. His peers all insisted that salvation was at hand. To go to a Doors concert was to stare at the lithe messiah undressing on stage and believe that it was entirely possible to break on through to the other side. To see Cohen play was to gawk at an aging Jew telling you that life was hard and laced with sorrow but that if we love each other and fuck one another and have the mad courage to laugh even when the sun is clearly setting, we’ll be just all right. To borrow a metaphor from a field never too far from Cohen’s heart, theology, Morrison, Hendrix, Joplin, and the rest were all good Christians, and they set themselves up as the redeemers who had to die for the sins of their fans. Cohen was a Jew, and like Jews he believed that salvation was nothing more than a lot of hard work and a small but sustainable reward.&quot;\n",
      "\n",
      "{'neg': 0.111, 'neu': 0.724, 'pos': 0.166, 'compound': 0.9177}\n",
      "--------\n",
      "I saw Leonard Cohen on his last tour. He opened every show on the tour by saying &quot;I don’t know if we’ll meet again, but tonight we’ll give you everything we got.&quot;\n",
      "\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "--------\n",
      "I&#x27;m not sad. He had probably the best life he could have had, and it ended on a high note.\n",
      "\n",
      "{'neg': 0.0, 'neu': 0.716, 'pos': 0.284, 'compound': 0.7753}\n",
      "--------\n",
      "<a href=\"http:&#x2F;&#x2F;www.rollingstone.com&#x2F;music&#x2F;news&#x2F;leonard-cohen-pens-final-letter-to-so-long-marianne-muse-w433144\" rel=\"nofollow\">http:&#x2F;&#x2F;www.rollingstone.com&#x2F;music&#x2F;news&#x2F;leonard-cohen-pens-fi...</a><p>Back in July he wrote to a dying Marianne Ihlen, &quot;Know that I am so close behind you that if you stretch out your hand, I think you can reach mine.&quot;\n",
      "\n",
      "{'neg': 0.0, 'neu': 0.867, 'pos': 0.133, 'compound': 0.5106}\n",
      "--------\n",
      "This article was posted here about a month ago:<p><pre><code> http:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;2016&#x2F;10&#x2F;17&#x2F;leonard-cohen-makes-it-darker </code></pre> Discussion:<p><pre><code> https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12700141 </code></pre> Interestingly, I was thinking about him on the subway just before surfacing to see above thread pop up. Very glad that I got to catch up on his bio (and to see him live a few years back) before his departure.<p>2016 - what a year.\n",
      "\n",
      "{'neg': 0.0, 'neu': 0.895, 'pos': 0.105, 'compound': 0.7178}\n",
      "--------\n",
      "Sad for the artistic loss but also glad he died at peace after a rich life spent doing what he loved till the last moment. He joins a special list, alongside Hintjens who also passed recently, of those who manage to strip the dread from death and stress the importance of &#x27;tidying up&#x27; over passive acceptance as one enters the final days.<p><i>“The big change is the proximity to death,” he said. “I am a tidy kind of guy. I like to tie up the strings if I can. If I can’t, also, that’s O.K. But my natural thrust is to finish things that I’ve begun.”</i><p><i>“For some odd reason,” he went on, “I have all my marbles, so far. I have many resources, some cultivated on a personal level, but circumstantial, too: my daughter and her children live downstairs, and my son lives two blocks down the street. So I am extremely blessed. I have an assistant who is devoted and skillful. I have a friend like Bob and another friend or two who make my life very rich. So in a certain sense I’ve never had it better. . . . At a certain point, if you still have your marbles and are not faced with serious financial challenges, you have a chance to put your house in order. It’s a cliché, but it’s underestimated as an analgesic on all levels. Putting your house in order, if you can do it, is one of the most comforting activities, and the benefits of it are incalculable.”</i> [0]<p>[0] <a href=\"http:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;2016&#x2F;10&#x2F;17&#x2F;leonard-cohen-makes-it-darker\" rel=\"nofollow\">http:&#x2F;&#x2F;www.newyorker.com&#x2F;magazine&#x2F;2016&#x2F;10&#x2F;17&#x2F;leonard-cohen-m...</a>\n",
      "\n",
      "{'neg': 0.101, 'neu': 0.64, 'pos': 0.259, 'compound': 0.9949}\n",
      "--------\n",
      "I took my mum to see him live in Brisbane back in 2013. At the end of the show he thanked his backing band. And then he thanked, by name, the sound engineers, the lighting operators, the cameraman filming for the tour DVD, and various other staff. One of the greatest musicians to have lived but also a genuine and decent person.\n",
      "\n",
      "{'neg': 0.0, 'neu': 0.881, 'pos': 0.119, 'compound': 0.6757}\n",
      "--------\n",
      "I believe we recently discussed Mr. Cohen: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12700141\" rel=\"nofollow\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12700141</a><p>I have cried more tears listening to Leonard Cohen than all the other tears I&#x27;ve cried combined, his music, his words, his poems have always resonated deeply within me. He truly is my favourite artist. We listened to him daily in my dad&#x27;s house and I grew to find an incredibly amount of peace in his voice. Love the HN community seems to like him as much. rest well sir.<p><a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4MXOuaZuTak\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=4MXOuaZuTak</a>\n",
      "\n",
      "{'neg': 0.1, 'neu': 0.729, 'pos': 0.171, 'compound': 0.8333}\n",
      "--------\n",
      "So sad. I saw him in Manhattan. My wife got tickets as a Christmas present, knowing I had been excited when we were traveling in Barcelona and he was there that week. (Couldn&#x27;t get tickets -- didn&#x27;t even try.) So we showed up at Madison Square Garden and I had no idea what we were seeing -- and there were no markings to give away the surprise! It was not until the show started that I knew it was a Leonard Cohen concert. It was an awesome evening.<p><pre><code> If you want a lover I&#x27;ll do anything you ask me to And if you want another kind of love I&#x27;ll wear a mask for you If you want a partner, take my hand, or If you want to strike me down in anger Here I stand I&#x27;m your man</code></pre>\n",
      "\n",
      "{'neg': 0.086, 'neu': 0.748, 'pos': 0.166, 'compound': 0.8752}\n",
      "--------\n",
      "And then you heard, on Remembrance Day, of the Poet who had gone away; No more music in the dark to keep you warm.<p>For years to come you will recall the music&#x27;s death, the soldier&#x27;s fall, and your songs salute them both. So: Hallelujah!<p>Hallelujah! Hallelujah! Hallelujah! Hallelujah!<p>--<p>The first Cohen song that I ever heard was &quot;Everybody Knows&quot;, in <i>Pump Up the Volume</i>.\n",
      "\n",
      "{'neg': 0.084, 'neu': 0.783, 'pos': 0.134, 'compound': 0.6833}\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "print('--------')\n",
    "for filename in glob.glob(\"../Data/LCohen/*.txt\"): \n",
    "    with open(filename, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "        scores = sid.polarity_scores(content)\n",
    "        print(content)\n",
    "        print(scores)\n",
    "        print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# You can copy the file name of the most positive comment here\n",
    "12926915.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# You can copy the file name of the most negative comment here\n",
    "12926778.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Getting the list of top-N (instead of top-1) comments that are positive/negative is much more challenging and requires additional lists and dictionaries to store the data and order it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "There is a lot of interesting data online. For instance, the [Hackernews API](https://github.com/HackerNews/API) is a nice source of discussions between people, and of opiniated text. We are going to retrieve some posts of:\n",
    "* Stephen Merity: a machine learning researcher who also has a nice tech blog ([JSON link here](https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty))\n",
    "* Gabriel Weinberg: founder of the DuckDuckGo search engine ([JSON link here](https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty))\n",
    "\n",
    "We expect that Stephen will talk more about technology, and Gabriel will talk more about the Web.\n",
    "\n",
    "Since we did not teach you how to download data, we are providing you with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As another Australian, our housing prices is broken for multiple reasons but foreign investment is a _major_ contributor.<p>Credit Suisse research indicates that &quot;foreigners are acquiring 25 per cent of newly completed supply in NSW and 16 per cent in Melbourne, or 21 per cent if we combine the two states&quot;. The total value of new houses in both states was $39 billion over the relevant 12 months.<p>After the Chinese government cracked down on monetary restrictions (i.e. Citizens of China can normally only convert US$50,000 a year in foreign currency and have long been barred from buying property overseas), Lend Lease reported 30 to 40% of foreign purchases now being cash settled.<p>Transparency International consider Australia the worst money laundering property market in the world.<p>Foreign investment, especially for countries which avoided the 2007 housing bubble such as Australia, is a major issue.<p>A friend will be posting an article on the multiple bubbles he sees in the Australian economy, and property is potentially the most concerning of all given how exposed our banks are to housing loans.\n",
      "We may find Australia finally seeing their house bubble pop like the US in 2007.<p><a href=\"http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-cheap-for-chinese-buyers-credit-suisse-20170323-gv5ets.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-...</a><p><a href=\"https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide_open_corruption_and_real_estate_in_four_key_markets\" rel=\"nofollow\">https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide...</a><p>Updated to include direct link to Credit Suisse: <a href=\"https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;format=PDF&amp;sourceid=csplusresearchcp&amp;document_id=1072773781&amp;serialid=vGaaRmOdA9BYZyKVi%2BOQvCfX%2BJZ8vpuaqcWf8o4NoF8%3D\" rel=\"nofollow\">https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;...</a>\n",
      "Amazon offer an official AMI which comes preloaded with various deep learning frameworks: MXNet, TensorFlow, CNTK, Caffe&#x2F;2, Theano, Torch and Keras.<p>For the P3 (Volta V100) instances you&#x27;ll want to ensure you use an AMI preloaded with CUDA 9, though not all DL frameworks are happy with that yet.<p><a href=\"https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;</a>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty\"\n",
    "r_user=requests.get(download_user_uri)\n",
    "user_data = r_user.json()\n",
    "#print(user_data)\n",
    "items = user_data['submitted']\n",
    "for item in items[:2]: # download the first two items\n",
    "    download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "    r_text = requests.get(download_item_uri)\n",
    "    item_data = r_text.json()\n",
    "    content=item_data['text']\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above works and gives us two articles written by the author \"Smerity\". Good! But if we want to download, let's say, five articles written by another user now, we would have to copy and edit the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks Andrew -- appreciate it! I shudder to think of all the things I posted 8 years ago :).\n",
      "It is not only useful on our platform. The entire instant answer engine is open source as well, and could be incorporated in any program. Additionally, much of our Instant Answers are freely accessible via our API: <a href=\"https:&#x2F;&#x2F;duckduckgo.com&#x2F;api\" rel=\"nofollow\">https:&#x2F;&#x2F;duckduckgo.com&#x2F;api</a>. These could be and are incorporated in other programs. We are looking for ways to expand access and make them more useful, and would appreciate any suggestions.<p>That said, there are many for-profit companies that create and maintain open source projects that are primarily used for them. While that may mean that many people aren&#x27;t interested in developing for them, it doesn&#x27;t mean everyone isn&#x27;t. There are some people who want to improve their own search results. There are some people who just want to help a private alternative in search.<p>There are others who would like exposure on our search engine pages (for their APIs or via developer attribution). In fact, as we get a decent amount of traffic now, that exposure has real monetary value. We place Instant Answers above ads and links, and so the sources that are used for Instant Answers are monetarily benefiting through their inclusion in the ecosystem. In any case, there are many valid reasons why someone may want to develop on DuckDuckHack.\n",
      "We would <i>love</i> your help: <a href=\"http:&#x2F;&#x2F;duckduckhack.com&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;duckduckhack.com&#x2F;</a><p>DuckDuckHack (that link) is our instant answer platform and it is all open source (both the answers themselves and the underlying answer engine). We believe that each programming area (MySQL, Perl, React, etc.) can be best served by a group of Instant Answers, and only developers with expertise in those areas can craft the best experiences for that set of Instant Answers.<p>As a small team, we therefore need help in our goal of making DuckDuckGo the best search engine for developers.\n",
      "DuckDuckGo - Remote<p><a href=\"https:&#x2F;&#x2F;duck.co&#x2F;help&#x2F;company&#x2F;hiring\" rel=\"nofollow\">https:&#x2F;&#x2F;duck.co&#x2F;help&#x2F;company&#x2F;hiring</a>\n",
      "Thanks! I would too love to spread those stories to new audiences. We are really trying to branch out beyond the startup world but are honestly new to it and unsure so far what will work. So any ideas from you or anyone would be appreciated! Open to try almost anything :). Also, unjust sent you a DM on Twitter about a related matter.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty\"\n",
    "r_user=requests.get(download_user_uri)\n",
    "user_data = r_user.json()\n",
    "#print(user_data)\n",
    "items = user_data['submitted']\n",
    "for item in items[:5]: # download the first five items\n",
    "    download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "    r_text = requests.get(download_item_uri)\n",
    "    item_data = r_text.json()\n",
    "    content=item_data['text']\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "We can see that a lot of the code is now repeated between the two code blocks. This is called \"spaghetti code\" and it is a very bad practice. Luckily, we learned about functions and we can apply them to *refactor* the code.\n",
    "\n",
    "* **5a** Create a function `download_user(user, max_items)` that will allow us to download the posts by any user just by supplying its username to this function. The parameter `max_items` lets us choose how many articles to download from each user.** \n",
    "* **5b** Use these two parameters inside the function to replace the fixed values of \"Smerity\" (for user) and 2 (for maximum items). The parameter `user` should be a positional parameter, while `max_items` should be a keyword parameter with a default value of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty\n",
      "As another Australian, our housing prices is broken for multiple reasons but foreign investment is a _major_ contributor.<p>Credit Suisse research indicates that &quot;foreigners are acquiring 25 per cent of newly completed supply in NSW and 16 per cent in Melbourne, or 21 per cent if we combine the two states&quot;. The total value of new houses in both states was $39 billion over the relevant 12 months.<p>After the Chinese government cracked down on monetary restrictions (i.e. Citizens of China can normally only convert US$50,000 a year in foreign currency and have long been barred from buying property overseas), Lend Lease reported 30 to 40% of foreign purchases now being cash settled.<p>Transparency International consider Australia the worst money laundering property market in the world.<p>Foreign investment, especially for countries which avoided the 2007 housing bubble such as Australia, is a major issue.<p>A friend will be posting an article on the multiple bubbles he sees in the Australian economy, and property is potentially the most concerning of all given how exposed our banks are to housing loans.\n",
      "We may find Australia finally seeing their house bubble pop like the US in 2007.<p><a href=\"http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-cheap-for-chinese-buyers-credit-suisse-20170323-gv5ets.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-...</a><p><a href=\"https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide_open_corruption_and_real_estate_in_four_key_markets\" rel=\"nofollow\">https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide...</a><p>Updated to include direct link to Credit Suisse: <a href=\"https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;format=PDF&amp;sourceid=csplusresearchcp&amp;document_id=1072773781&amp;serialid=vGaaRmOdA9BYZyKVi%2BOQvCfX%2BJZ8vpuaqcWf8o4NoF8%3D\" rel=\"nofollow\">https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;...</a>\n",
      "Amazon offer an official AMI which comes preloaded with various deep learning frameworks: MXNet, TensorFlow, CNTK, Caffe&#x2F;2, Theano, Torch and Keras.<p>For the P3 (Volta V100) instances you&#x27;ll want to ensure you use an AMI preloaded with CUDA 9, though not all DL frameworks are happy with that yet.<p><a href=\"https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;</a>\n",
      "https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty\n",
      "Thanks Andrew -- appreciate it! I shudder to think of all the things I posted 8 years ago :).\n",
      "It is not only useful on our platform. The entire instant answer engine is open source as well, and could be incorporated in any program. Additionally, much of our Instant Answers are freely accessible via our API: <a href=\"https:&#x2F;&#x2F;duckduckgo.com&#x2F;api\" rel=\"nofollow\">https:&#x2F;&#x2F;duckduckgo.com&#x2F;api</a>. These could be and are incorporated in other programs. We are looking for ways to expand access and make them more useful, and would appreciate any suggestions.<p>That said, there are many for-profit companies that create and maintain open source projects that are primarily used for them. While that may mean that many people aren&#x27;t interested in developing for them, it doesn&#x27;t mean everyone isn&#x27;t. There are some people who want to improve their own search results. There are some people who just want to help a private alternative in search.<p>There are others who would like exposure on our search engine pages (for their APIs or via developer attribution). In fact, as we get a decent amount of traffic now, that exposure has real monetary value. We place Instant Answers above ads and links, and so the sources that are used for Instant Answers are monetarily benefiting through their inclusion in the ecosystem. In any case, there are many valid reasons why someone may want to develop on DuckDuckHack.\n",
      "We would <i>love</i> your help: <a href=\"http:&#x2F;&#x2F;duckduckhack.com&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;duckduckhack.com&#x2F;</a><p>DuckDuckHack (that link) is our instant answer platform and it is all open source (both the answers themselves and the underlying answer engine). We believe that each programming area (MySQL, Perl, React, etc.) can be best served by a group of Instant Answers, and only developers with expertise in those areas can craft the best experiences for that set of Instant Answers.<p>As a small team, we therefore need help in our goal of making DuckDuckGo the best search engine for developers.\n"
     ]
    }
   ],
   "source": [
    "def download_user(user, max_items = 5):\n",
    "    # Your code here\n",
    "    # don't forget to replace the mentions of the user and maximum items with the parameters of the function to make it general\n",
    "    download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/\" + user + \".json?print=pretty\"\n",
    "    print(download_user_uri)\n",
    "    r_user=requests.get(download_user_uri)\n",
    "    user_data = r_user.json()\n",
    "    items = user_data['submitted']\n",
    "    for item in items[:max_items]: # download the first five items\n",
    "        download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "        r_text = requests.get(download_item_uri)\n",
    "        item_data = r_text.json()\n",
    "        content=item_data['text']\n",
    "        print(content)\n",
    "    \n",
    "    \n",
    "# Now let's test this function\n",
    "merity = download_user('Smerity', 2)\n",
    "weinberg = download_user('epi0Bauqu', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** \n",
    "\n",
    "Now that we have removed some unnecesary repetition and gained flexibility, we could do another cycle of refactoring (note that for instance the 3 lines used to download user and item info are also similar). But instead, let's move on and prepare this data for linguistic processing:\n",
    "\n",
    "\n",
    "* **7a** create a list within the function called `contents`. Put every text article in this list and store the articles in a JSON in the end of the function. This JSON should look as follows:\n",
    "`{\"Smerity\":[\"this is the text of article 1\", \"this is the text of article 2\", ..., \"this is the text of article 10\"]}`.\n",
    "And similarly for the other user.\n",
    "* **7b** Now call this function and retrieve 10 texts per user.\n",
    "* **7c** Store the JSON dictionaries for both users in two separate JSON files \"../Data/Hackernews/cache_Smerity.json\" and \"../Data/Hackernews/cache_epi0Bauqu.json\". This is useful because your analysis will be replicable (maybe a user will delete their posts, or start posting about something completely different), and you won't have to download the data again. If this directory does not exist, create it.\n",
    "\n",
    "*Hint \\#1: Use the `os` module to create a directory if it does not exist.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hacker-news.firebaseio.com/v0/user/Smerity.json?print=pretty\n",
      "{'Smerity': ['As another Australian, our housing prices is broken for multiple reasons but foreign investment is a _major_ contributor.<p>Credit Suisse research indicates that &quot;foreigners are acquiring 25 per cent of newly completed supply in NSW and 16 per cent in Melbourne, or 21 per cent if we combine the two states&quot;. The total value of new houses in both states was $39 billion over the relevant 12 months.<p>After the Chinese government cracked down on monetary restrictions (i.e. Citizens of China can normally only convert US$50,000 a year in foreign currency and have long been barred from buying property overseas), Lend Lease reported 30 to 40% of foreign purchases now being cash settled.<p>Transparency International consider Australia the worst money laundering property market in the world.<p>Foreign investment, especially for countries which avoided the 2007 housing bubble such as Australia, is a major issue.<p>A friend will be posting an article on the multiple bubbles he sees in the Australian economy, and property is potentially the most concerning of all given how exposed our banks are to housing loans.\\nWe may find Australia finally seeing their house bubble pop like the US in 2007.<p><a href=\"http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-cheap-for-chinese-buyers-credit-suisse-20170323-gv5ets.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.smh.com.au&#x2F;business&#x2F;property&#x2F;australian-property-...</a><p><a href=\"https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide_open_corruption_and_real_estate_in_four_key_markets\" rel=\"nofollow\">https:&#x2F;&#x2F;www.transparency.org&#x2F;whatwedo&#x2F;publication&#x2F;doors_wide...</a><p>Updated to include direct link to Credit Suisse: <a href=\"https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;format=PDF&amp;sourceid=csplusresearchcp&amp;document_id=1072773781&amp;serialid=vGaaRmOdA9BYZyKVi%2BOQvCfX%2BJZ8vpuaqcWf8o4NoF8%3D\" rel=\"nofollow\">https:&#x2F;&#x2F;research-doc.credit-suisse.com&#x2F;docView?language=ENG&amp;...</a>', 'Amazon offer an official AMI which comes preloaded with various deep learning frameworks: MXNet, TensorFlow, CNTK, Caffe&#x2F;2, Theano, Torch and Keras.<p>For the P3 (Volta V100) instances you&#x27;ll want to ensure you use an AMI preloaded with CUDA 9, though not all DL frameworks are happy with that yet.<p><a href=\"https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aws.amazon.com&#x2F;amazon-ai&#x2F;amis&#x2F;</a>', 'Honestly, I didn&#x27;t spend enough time delving in to the Python overhead, especially in terms of the framework. Most of it would be an issue of my own causing however rather than the framework&#x27;s. The original code I wrote was never written with data loading &#x2F; saving in mind as the source for speed issues so I avoided what would have been premature optimization at the time.<p>Some of the slowdowns now just seem silly and aren&#x27;t even listed in the per epoch timings: PyTorch doesn&#x27;t have an asynchronous torch.save(). This means that if you save your model after each epoch, and the model save takes a few seconds, you&#x27;re increasing your per epoch timings 5-10% just by saving the damn thing!<p>Regarding FP16, PyTorch supports, and there&#x27;s even a pull request that updates the examples repo with FP16 support for language modeling and ImageNet. It&#x27;s not likely to be merged as it greatly complicates a codebase that&#x27;s meant primarily for teaching purposes but it&#x27;s lovely to look at. I also think many of the FP16 issues will get a general wrapper and they&#x27;ll become far more agnostic to the end user. For the most part they&#x27;re all outlined in NVIDIA &#x2F; Baidu&#x27;s &quot;Mixed Precision Training&quot; paper. Might be useful for DeepLearning4j to go through the most common heavy throughput use cases and get them running (just as an example of how to work around issues really) if customers were using P100s&#x2F;V100s?<p>I&#x27;m really interested in exploring the FP16 aspect as the QRNN, especially for single GPU, is sitting at basically 100% utilization, with almost all the time spent on matrix multiplications. FP16 is about the only way to speed it up at that stage. This gets a tad more complicated regardless as the CUDA kernel is not written in FP16 (and is not easy to do so) but even converting FP16-&gt;FP32-&gt;(QRNN element-wise CUDA kernel)-&gt;FP16 (&quot;pseudo&quot; FP16) should still be a crazy speedup. I tested that on the P100 and it took per epoch AWD-QRNN from ~28 seconds to ~18.<p>- PyTorch async save issue: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;1567\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;1567</a><p>- PyTorch FP16 examples pull request: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;examples&#x2F;pull&#x2F;203\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;examples&#x2F;pull&#x2F;203</a><p>- &quot;Mixed Precision Training&quot;: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1710.03740\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1710.03740</a>', 'The P3 instances are the first widely and easily accessible machines that use the NVIDIA Tesla V100 GPUs. These GPUs are straight up scary in terms of firepower.\\nTo give an understanding of the speed-up compared to the P2 instances for a research project of mine:<p>+ P2 (K80) with single GPU: ~95 seconds per epoch<p>+ P3 (V100) with single GPU: ~20 seconds per epoch<p>Admittedly this isn&#x27;t exactly fair for either GPU - the K80 cards are straight up ancient now and the Volta isn&#x27;t sitting at 100% GPU utilization as it burns through the data too quickly ([CUDA kernel, Python] overhead suddenly become major bottlenecks).\\nThis gives you an indication of what a leap this is if you&#x27;re using GPUs on AWS however.\\nOh, and the V100 comes with 16GB of (faster) RAM compared to the K80&#x27;s 12GB of RAM, so you win there too.<p>For anyone using the standard set of frameworks (Tensorflow, Keras, PyTorch, Chainer, MXNet, DyNet, DeepLearning4j, ...) this type of speed-up will likely require you to do nothing - except throw more money at the P3 instance :)<p>If you really want to get into the black magic of speed-ups, these cards also feature full FP16 support, which means you can double your TFLOPS by dropping to FP16 from FP32. You&#x27;ll run into a million problems during training due to the lower precision but these aren&#x27;t insurmountable and may well be worth the pain for the additional speed-up &#x2F; better RAM usage.<p>- Good overview of Volta&#x27;s advantages compared to event the recent P100: <a href=\"https:&#x2F;&#x2F;devblogs.nvidia.com&#x2F;parallelforall&#x2F;inside-volta&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;devblogs.nvidia.com&#x2F;parallelforall&#x2F;inside-volta&#x2F;</a><p>- Simple table comparing V100 &#x2F; P100 &#x2F; K40 &#x2F; M40: <a href=\"https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;11367&#x2F;nvidia-volta-unveiled-gv100-gpu-and-tesla-v100-accelerator-announced\" rel=\"nofollow\">https:&#x2F;&#x2F;www.anandtech.com&#x2F;show&#x2F;11367&#x2F;nvidia-volta-unveiled-g...</a><p>- NVIDIA&#x27;s V100 GPU architecture white paper: <a href=\"http:&#x2F;&#x2F;www.nvidia.com&#x2F;object&#x2F;volta-architecture-whitepaper.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.nvidia.com&#x2F;object&#x2F;volta-architecture-whitepaper.h...</a><p>- The numbers above were using my PyTorch code at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm</a> and the Quasi-Recurrent Neural Network (QRNN) at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;pytorch-qrnn\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;pytorch-qrnn</a> which features a custom CUDA kernel for speed', 'Whilst it might be a general concept, the most recent time I heard your paraphrased notes on warm introductions was Marc Andreessen at Startup School.<p>Both the video and the transcript (with the warm introduction question first) are available at:<p><a href=\"http:&#x2F;&#x2F;blog.ycombinator.com&#x2F;marc-andreessen-at-startup-school&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;blog.ycombinator.com&#x2F;marc-andreessen-at-startup-schoo...</a>', 'Facebook are getting straight up annoying when it comes to their ads. The worst for me is their in feed video player. When an ad appears on a video, you can&#x27;t like, comment, or share the video[1]. You&#x27;re going to break my flow of using your own site so you can giddily inform advertisers that I paid more attention to their ad?<p>I think Facebook have rapidly transitioned to an ad first, rather than user first, experience.<p>[1]: Screenshot - <a href=\"http:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;hCxOT\" rel=\"nofollow\">http:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;hCxOT</a>', 'In practice, images are not particularly large and a batch of them would easily fit on a single GPU. What&#x27;s more common is either (a) performing the forward and backward passes on 4 GPUs where each GPU has its own batch, then collecting the gradient from all 4 backward passes or (b) splitting the computation for individual layers across multiple GPUs.<p>Both (a) and (b) have various trade-offs. Some models perform worse with large batch sizes, so (a) is not preferred, and others are hard or impossible to parallelize at the layer level, ruling out (b). Google NMT did (b), though it required many trade-offs and restrictions (see my blog post[1]), while many image based tasks are happy with large batch sizes so go with (a).<p>[1]: <a href=\"http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html\" rel=\"nofollow\">http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html</a>', 'Hey Aidan,<p>As noted, it&#x27;s not a criticism of a reading list tailored for you, I simply feel it&#x27;s a really confusing reading list given the likely audience that will arrive there from Hacker News and the title. I also don&#x27;t think it can be pieced together to form a good introductory reading list or that the contents as stated are &quot;foundational&quot; yet given the progression and holes.<p>Your (1) confuses me still - I admittedly don&#x27;t know how you got started in mathematics or how that&#x27;s relevant here?<p>Regarding (2), there certainly is a place for neuroscience to influencing thinking or introduce new ideas, and I&#x27;m certainly not disregarding the entirety of that potential intersection, but it&#x27;s a very specific field that is still largely disconnected from the practical application of neural networks. This may change, slightly or substantially, in the future, but thinking again with the lens of the audience of Hacker News (where the incorrect adages of the style &quot;deep learning learns just like a human brain&quot; gets thrown around frequently), I rarely want to exaggerate the influence that neuroscience has on the field at this stage. I&#x27;d also lightly note that appealing to authority isn&#x27;t an argument, though do note the author&#x27;s pedigree.', 'With all due respect, this is a quite random reading list, and appears more a result of the &#x27;if &quot;deep learning&quot; in post.title: post.upvote()&#x27; trend on Hacker News ...<p>I will pick on two under the &quot;Classics&quot; section simply as I know the author and have used their work, so am not in any way saying the work isn&#x27;t useful (it certainly can be in the right spot!), but it isn&#x27;t &quot;classic&quot; or what I&#x27;d recommend for early readers at all.\\n&quot;Uncertainty in Deep Learning&quot; and &quot;Dropout as a Bayesian Approximation&quot; were both published within the last year and a half and is a PhD thesis + paper on interpretations of neural networks in a Bayesian fashion. &quot;Classic&quot; for a paper + thesis less than two years old is quite a stretch even for the fast moving field of deep learning.\\nThe same holds true for many of the other papers in &quot;Classics&quot; such as batch norm which is (a) recent and (b) certainly not the only of such techniques (see layer norm, recurrent batch norm, ...) and (c) has complications in implementation[1].<p>As the simplest example, why is the original dropout paper[7] not under classics? It&#x27;s an elegant paper, fundamentally important for current neural networks, and is more classic than &quot;Dropout as a Bayesian Approximation&quot; or &quot;Dropout Rademacher Complexity of Deep Neural Networks&quot; which are both listed.<p>I&#x27;m also highly dubious of the noted neuroscience connection - most deep learning researchers use very little from neuroscience.<p>Again, this list may be helpful to the creator of the repo and tailored toward their specific research direction but it is not useful for readers from Hacker News or those aiming to get their start in deep learning. Why so many upvotes? Zero comments? Zero discussion?<p>If you want a book, check out the Deep Learning book[2]. If you want a course for RNNs, check out CS224d[3]. If you want a course for CNNs, check out CS231n[4]. If you want to get down and dirty in a practical software engineering way, check out Fast AI[5]. If you want summaries of select recent deep learning papers in GitHub format, check out Denny Britz&#x27;s notes[8]. There are many other starting points but those are my default suggestions.<p>If you really want to start learning, this isn&#x27;t the right list for you and I&#x27;d really like to suggest a more sane and potentially tailored path. Seriously. If you reply with what you want, I&#x27;ll do my best to suggest a starting point.<p>Background: I&#x27;m a deep learning researcher who publishes papers and articles[6].<p>[1]: <a href=\"http:&#x2F;&#x2F;www.alexirpan.com&#x2F;2017&#x2F;04&#x2F;26&#x2F;perils-batch-norm.html\" rel=\"nofollow\">http:&#x2F;&#x2F;www.alexirpan.com&#x2F;2017&#x2F;04&#x2F;26&#x2F;perils-batch-norm.html</a><p>[2]: <a href=\"http:&#x2F;&#x2F;www.deeplearningbook.org&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;www.deeplearningbook.org&#x2F;</a><p>[3]: <a href=\"http:&#x2F;&#x2F;cs224d.stanford.edu&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;cs224d.stanford.edu&#x2F;</a><p>[4]: <a href=\"http:&#x2F;&#x2F;cs231n.github.io&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;cs231n.github.io&#x2F;</a><p>[5]: <a href=\"http:&#x2F;&#x2F;course.fast.ai&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;course.fast.ai&#x2F;</a><p>[6]: <a href=\"http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html\" rel=\"nofollow\">http:&#x2F;&#x2F;smerity.com&#x2F;articles&#x2F;2016&#x2F;google_nmt_arch.html</a><p>[7]: <a href=\"https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;JMLRdropout.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~hinton&#x2F;absps&#x2F;JMLRdropout.pdf</a><p>[8]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;dennybritz&#x2F;deeplearning-papernotes\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dennybritz&#x2F;deeplearning-papernotes</a>', 'I presume you&#x27;re already familiar with computing the numerical and analytical Jacobian[1][2] and just wishing for a better way? :) They&#x27;re memory intensive as all hell and pretty finicky but at least it&#x27;s something. I&#x27;ll admit that when floating point calculations are involved it can all go to hell anyway.<p>Recently I had to implement gradient calculations by hand recently (writing custom CUDA code) and had a pretty terrible time. Mixing the complications of CUDA code with my iffy manual differentiations and floating point silliness can drive you a little bonkers. I ended up implementing a slow automatic differentiated version and compared resulting outputs and gradients to help work through my bugs.<p>Here&#x27;s hoping that Tensorflow&#x27;s XLA and other JIT style CUDA compilers&#x2F;optimizers will make much of this obsolete in the near future.<p>For those not familiar, the overhead for calling a CUDA kernel can be insanely high, especially when you&#x27;re just doing an elementwise operation such as an add. Given your neural network likely has many many of these, wrapping many of these into one small piece of custom CUDA can result in substantial speed increases. Unfortunately there&#x27;s not really any automatic way of doing that yet. We&#x27;re stuck in the days of either writing manual assembly or being fine with suboptimal compiled C.<p>[1]: <a href=\"https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.11&#x2F;api_docs&#x2F;python&#x2F;test&#x2F;gradient_checking\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tensorflow.org&#x2F;versions&#x2F;r0.11&#x2F;api_docs&#x2F;python&#x2F;te...</a><p>[2]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;autograd&#x2F;gradcheck.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;blob&#x2F;master&#x2F;torch&#x2F;autogra...</a>']}\n",
      "https://hacker-news.firebaseio.com/v0/user/epi0Bauqu.json?print=pretty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epi0Bauqu': ['Thanks Andrew -- appreciate it! I shudder to think of all the things I posted 8 years ago :).', 'It is not only useful on our platform. The entire instant answer engine is open source as well, and could be incorporated in any program. Additionally, much of our Instant Answers are freely accessible via our API: <a href=\"https:&#x2F;&#x2F;duckduckgo.com&#x2F;api\" rel=\"nofollow\">https:&#x2F;&#x2F;duckduckgo.com&#x2F;api</a>. These could be and are incorporated in other programs. We are looking for ways to expand access and make them more useful, and would appreciate any suggestions.<p>That said, there are many for-profit companies that create and maintain open source projects that are primarily used for them. While that may mean that many people aren&#x27;t interested in developing for them, it doesn&#x27;t mean everyone isn&#x27;t. There are some people who want to improve their own search results. There are some people who just want to help a private alternative in search.<p>There are others who would like exposure on our search engine pages (for their APIs or via developer attribution). In fact, as we get a decent amount of traffic now, that exposure has real monetary value. We place Instant Answers above ads and links, and so the sources that are used for Instant Answers are monetarily benefiting through their inclusion in the ecosystem. In any case, there are many valid reasons why someone may want to develop on DuckDuckHack.', 'We would <i>love</i> your help: <a href=\"http:&#x2F;&#x2F;duckduckhack.com&#x2F;\" rel=\"nofollow\">http:&#x2F;&#x2F;duckduckhack.com&#x2F;</a><p>DuckDuckHack (that link) is our instant answer platform and it is all open source (both the answers themselves and the underlying answer engine). We believe that each programming area (MySQL, Perl, React, etc.) can be best served by a group of Instant Answers, and only developers with expertise in those areas can craft the best experiences for that set of Instant Answers.<p>As a small team, we therefore need help in our goal of making DuckDuckGo the best search engine for developers.', 'DuckDuckGo - Remote<p><a href=\"https:&#x2F;&#x2F;duck.co&#x2F;help&#x2F;company&#x2F;hiring\" rel=\"nofollow\">https:&#x2F;&#x2F;duck.co&#x2F;help&#x2F;company&#x2F;hiring</a>', 'Thanks! I would too love to spread those stories to new audiences. We are really trying to branch out beyond the startup world but are honestly new to it and unsure so far what will work. So any ideas from you or anyone would be appreciated! Open to try almost anything :). Also, unjust sent you a DM on Twitter about a related matter.', 'It more doesn&#x27;t work for our privacy policy since we don&#x27;t track users.', 'I&#x27;m not sure if we improved for Greek in particular yet but this generally (non English) has been a focus recently so it should really be improving. I&#x27;d be curious if you notice any improvement and if the region toggle does anything for you.', 'We&#x27;d love to become the default search wherever we can!', 'Oh man, double face palm :)<p>I would go around to local meetup groups here in Philly and continually show it to people, as well as friends and family.', 'These interviews (I referenced above) were really the first things I did: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;user&#x2F;tractionbook\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;user&#x2F;tractionbook</a><p>I blogged all of them on Hacker News (you can search the archives to see), and then started blogging ideas that would become the book and got feedback from those on my blog and on HN.<p>This was all before really writing. Once we actually started writing we recruited &quot;early readers&quot; periodically to read and give feedback up until we had a full draft more ready to go, in which case we hit up our whole list and got another round of fresh-eyes feedback.']}\n"
     ]
    }
   ],
   "source": [
    "def download_user(user, max_items = 5):\n",
    "    # Your code here\n",
    "    # don't forget to replace the mentions of the user and maximum items with the parameters of the function to make it general\n",
    "    download_user_uri=\"https://hacker-news.firebaseio.com/v0/user/\" + user + \".json?print=pretty\"\n",
    "    print(download_user_uri)\n",
    "    r_user=requests.get(download_user_uri)\n",
    "    user_data = r_user.json()\n",
    "    items = user_data['submitted']\n",
    "    contents = {}\n",
    "    contents[user]=[]\n",
    "    for item in items[:max_items]: # download the first five items\n",
    "        download_item_uri=\"https://hacker-news.firebaseio.com/v0/item/%s.json?print=pretty\" % item\n",
    "        r_text = requests.get(download_item_uri)\n",
    "        item_data = r_text.json()\n",
    "        content=item_data['text']\n",
    "        contents[user].append(content)\n",
    "    print(contents)\n",
    "    output_file = \"../Data/Hackernews/cache_\"+ user +\".json\" \n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        jsonString = json.dumps(contents) \n",
    "        outfile.write(jsonString)\n",
    "# Now let's test this function\n",
    "merity = download_user('Smerity', 10)\n",
    "weinberg = download_user('epi0Bauqu', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User profiling\n",
    "\n",
    "By downloading 10 articles from each user, we have practically created our first NLP corpus. We don't need to download *all* of their posts, a sample should be enough.\n",
    "\n",
    "**NOTE:** In case you had problems with downloading and storing the JSON files, you can download the two files directly from the `Data/Hackernews` folder on github.\n",
    "\n",
    "Let's see if we can do some user profiling, to judge what topics people like to talk about. As we said before, we expect that Stephen will talk more about technology, and Gabriel will talk more about the Web.\n",
    "\n",
    "We now need to **process the corpus** such that we can actually make a nice comparison between the two users. We will:\n",
    "\n",
    "* tokenize and lemmatize the posts by Merity and Weinberg.\n",
    "* Make two lists: `merity_lemmas` and `weinberg_lemmas`.\n",
    "* Use sets to determine the words *exclusively* used by Merity and Weinberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7** \n",
    "\n",
    "* **7a** Create a function that takes a string (the text of a comment), and returns a list of lemmas. Don't forget to use a docstring to say what the function does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "def text_to_lemmas(text):\n",
    "    \"\"\"Function to generate a lemma list for each post.\"\"\"\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    list_of_lemmas = []\n",
    "    # Lemmatization\n",
    "    lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for token, pos in tagged_tokens:\n",
    "        wn_tag = penn_to_wn(pos) # convert Penn Treebank POS tag to WordNet POS tag\n",
    "        if not wn_tag == None:\n",
    "            lemma = lmtzr.lemmatize(token, wn_tag)\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(token)\n",
    "        list_of_lemmas.append(lemma)\n",
    "    return list_of_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get this function (that generates lemmas for each post) for free :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_lemma_list(posts):\n",
    "    \"\"\"Function to generate a lemma list using a list of posts.\"\"\"\n",
    "    # This is the list that the function will use to collect all the lemmas.\n",
    "    lemma_list = []\n",
    "    for post in posts:\n",
    "    # First get the post\n",
    "        # Get the text from the post:\n",
    "        if 'text' in post:\n",
    "            text = post[\"text\"]\n",
    "        # Get the lemmas for the current text.\n",
    "        lemmas = text_to_lemmas(post)\n",
    "        # Extend the list with the lemmas found for the current text.\n",
    "        lemma_list.extend(lemmas)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7b** Now let's use JSON to load both user files. Try to put this also in a function, which returns a list of post texts. After that, we can generate lemma lists for both users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_user_from_json(user):\n",
    "    # function definition here\n",
    "    json_file = \"../Data/Hackernews/cache_\" + user + \".json\" \n",
    "    with open(json_file, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "        data  = json.loads(content)\n",
    "        posts = data[user]\n",
    "    return posts    \n",
    "    \n",
    "merity_posts=load_user_from_json(\"Smerity\")\n",
    "weinberg_posts=load_user_from_json(\"epi0Bauqu\")\n",
    "\n",
    "# Get all the lemmas \n",
    "merity_lemmas = generate_lemma_list(merity_posts)\n",
    "weinberg_lemmas = generate_lemma_list(weinberg_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some set operations, it's very easy to find words that are only used by one person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Facebook', 'drive', '100', 'prefer', 'Might', 'JMLRdropout.pdf', 'at', 'language', 'inside-volta', 'transition', 'heavy', '[', 'easily', 'advertiser', 'random', 'perform', '2016', 'cs231n.github.io', 'run', 'cent', '2', 'Aidan', 'indicate', 'crack', 'Bayesian', 'level', 'precision', 'nvidia-volta-unveiled-gv100-gpu-and-tesla-v100-accelerator-announced', 'find', 'magic', 'arrive', 'doors_wide_open_corruption_and_real_estate_in_four_key_markets', 'Seriously', 'NSW', '9', 'www.alexirpan.com', 'recurrent', 'pedigree', 'economy', 'throughput', 'obsolete', 'likely', 'PhD', 'slowdown', 'www.transparency.org', ']', '~18.', '3D', 'drop', 'fast', 'float', 'specific', 'preloaded', 'tailored', 'Google', 'speed-up', 'whatwedo', 'normally', 'buy', 'supply', 'size', 'such', 'rather', 'classic', 'least', 'major', 'during', 'problem', 'pay', 'Hey', 'argument', 'highly', 'disconnect', 'software', 'assembly', 'transcript', 'github.com', 'various', 'publish', 'too.', 'Keras', 'reading', 'complicated', 'Andreessen', 'www.anandtech.com', 'China', 'world.', 'examples', 'impossible', 'foundational', 'post.title', '6', 'however', 'asynchronous', 'sane', 'RNNs', 'form', 'question', 'adage', '2BOQvCfX', 'With', 'wrap', 'again', 'When', 'inform', 'machine', 'Admittedly', 'b', 'Networks', 'marc-andreessen-at-startup-schoo', 'style', 'Caffe', 'Denny', 'hard', 'request', 'after', 'future', 'also', 'rarely', 'hear', 'Mixing', 'house', 'except', 'frequently', 'Network', 'numerical', 'C.', 'Dropout', 'aim', 'CS224d', 'substantial', 'warm', 'timing', 'DL', 'train', 'learning', 'sit', 'white', 'customer', 'Recently', 'delve', 'If', 'stage', 're', 'finicky', 'language=ENG', 'Approximation', 'neuroscience', 'base', 'issue.', 'neuroscience.', 'analytical', 'code', 'respect', 'fine', 'NVIDIA', 'task', '$', 'format=PDF', '11367', 'when', 'black', 'hop', 'compiler', 'official', 'blog.ycombinator.com', 'money', 'state', 'Theano', 'tad', 'already', 'nothing', '4', 'www.cs.toronto.edu', 'document_id=1072773781', 'Marc', 'V100', 'pytorch', 'www.smh.com.au', 'widely', 'Australian', 'finally', 'kernel', 'total', 'feel', 'they', 'still', 'dropout', 'AI', 'epoch', 'reply', 'few', 'newly', 'Foreign', 'point.', 'comment', 'common', '~95', 'P2', 'end', 'terrible', 'creator', 'currency', 'half', 'hell', 'codebase', 'Most', 'python', 'nvidia-volta-unveiled-g', 'fair', 'stretch', 'very', '40', 'Startup', 'individual', 'For', 'come', 'Zero', 'feature', 'data', 'good', 'property', 'restriction', 'market', 'country', 'substantially', 'optimization', 'piece', 'Learning', 'optimizers', '16', 'US', 'Uncertainty', 'disregard', 'outline', 'doors_wide', 'www.nvidia.com', 'settled.', 'how', 'elegant', 'volta-architecture-whitepaper.html', 'version', 'potential', 'course', 'helpful', 'know', 'CS231n', 'dennybritz', 'suboptimal', 'docView', 'due', 'second', 'norm', 'influence', 'introduction', 'progression', 'DyNet', 'large', 'forward', 'relevant', 'thesis', 'issue', 'learn', 'recommend', 'interpretation', 'suddenly', 'batch', 'format', 'video', 'll', 'seem', 'section', 'break', 'M40', 'volta-architecture-whitepaper.h', '2007.', 'lightly', 'enough', 'implementation', 'operation', 'gradcheck.py', 'admittedly', 'connection', 'GPUs', 'GPU', 'Lend', 'gradient', 'V100s', 'Volta', 'intersection', 'discussion', 'framework', 'down', 'point', 'MXNet', 'upvotes', 'foreign', '2BJZ8vpuaqcWf8o4NoF8', 'Amazon', 'usage.', 'async', 'Suisse', 'Rademacher', 'particularly', 'autograd', 'RAM', 'AMI', 'Mixed', 'day', 'pain', 'purpose', 'greatly', 'te', 'basically', 'easy', 'Keras.', 'into', 'housing', 'crazy', 'bonkers', 'post.upvote', 'than', 'where', 'billion', 'certainly', 'pop', 'change', 'element-wise', 'GitHub', '+', 'AWD-QRNN', 'cash', 'leap', 'Melbourne', 'memory', 'explore', 'low', '5-10', 'PyTorch', 'check', 'silly', 'Simple', 'fundamentally', 'one', '203', 'Your', 'player', 'Complexity', 'compute', 'P3', 'Baidu', 'Transparency', 'holes.', 'authority', 'research', 'recent', 'silliness', 'FP32', 'image', 'paper', 'appear', 'title', 'layer', 'sourceid=csplusresearchcp', 'business', 'K80', 'quite', 'computation', 'general', 'To', 'largely', 'familiar', 'report', 's.', 'aws.amazon.com', 'ancient', 'summary', 'exactly', 'aspect', 'CNTK', 'absps', 'current', '5', 'smerity.com', 'table', 'complete', '...', 'c', 'wish', 'same', 'path', 'damn', 'collect', 'neural', 'important', 'elementwise', 'bottleneck', 'r0.11', 'lens', 'overview', 'After', 'quickly', 'torch', 'Good', 'might', 'right', 'calculation', '30', 'speedup', 'mind', '39', 'Honestly', 'International', 'Tensorflow', 'months.', 'iffy', 'he', 'Classic', 'causing', 'You', 'australian-property-', 'agnostic', 'toward', 'Unfortunately', 'firepower', 'trade-off', 'What', 'backward', 'content', 'concept', 'true', 'pull', 'confusing', 'introduce', 'awd-lstm-lm', 'support', 'launder', 'parallelize', 'additional', 'consider', 'autogra', '50,000', 'JIT', '21', 'FP16-', 'Here', 'these', 'GPUs.', 'overseas', '_major_', 'overhead', 'foreigner', 'bank', 'share', '~20', 'amazon-ai', 'multiple', 'Classics', 'introductory', 'Chainer', 'object', 'involve', 'mathematics', 'burn', 'starting', 'moving', 'DeepLearning4j', 'automatic', 'entirety', 'Why', 'last', 'me', 'australian-property-cheap-for-chinese-buyers-credit-suisse-20170323-gv5ets.html', 'note', 'anyway.', 'card', 'old', 'matrix', 'TensorFlow', 'differentiation', 'bar', 'paraphrased', 'premature', 'most', 'spot', 'little', 'trend', 'engineering', 'together', 'api_docs', 'time.', 'Deep', 'manual', 'Torch', 'lovely', 'complicate', 'brain', 'CUDA', 'compile', 'listed.', 'suggest', 'site', 'potentially', 'within', 'torch.save', 'bad', 'appeal', 'ImageNet', 'acquire', 'practice', 'differentiate', 'combine', 'convert', 'A', 'modeling', 'criticism', 'pass', 'article', 'fashion', 'noted', 'speed', 'under', 'throw', '~28', 'cs224d.stanford.edu', 'QRNN', 'bubble', 'spend', 'direction', 'slow', 'Updated', 'multiplication', 'TFLOPS', 'salesforce', 'application', 'deeplearning-papernotes', 'advantage', 'Australia', 'future.', '2007', 'Jacobian', 'simply', 'near', 'Both', 'K40', 'yet.', 'original', 'repo', 'an', 'part', 'They', 'merge', 'available', 'master', 'blob', 'two', 'i.e', 'example', 'Chinese', 'Regarding', 'win', 'P100s', 'select', 'either', 'across', 'avoid', 'CNNs', '7', 'incorrect', 'price', 'Training', 'google_nmt_arch.html', 'marc-andreessen-at-startup-school', 'Citizens', 'especially', 'require', 'loan', 'slightly', 'contributor.', 'amp', 'mine', '1710.03740', 'though', 'load', 'intensive', 'didn', 'model', 'something', 'never', 'gt', 'straight', 'even', 'complication', 'technique', 'presume', 'hand', 'giddily', 'compare', 'publication', 'FP32-', 'high', 'hold', 'researcher', '1567', 'less', 'rapidly', 'instance', 'annoy', 'implement', 'Python', 'wrapper', 'insanely', 'over', 'teach', 'pick', '12GB', '04', 'Tesla', 'pseudo', 'pretty', 'output', 'deep', 'per', 'test', 'insurmountable', 'course.fast.ai', 'purchase', 'simple', 'dubious', 'School.', 'happy', 'Credit', 'term', 'Screenshot', 'tailor', 'split', 'human', 'FP16', 'ensure', 'type', 'utilization', '25', 'time', 'confuses', 'admit', 'Neural', 'pytorch-qrnn', 'Some', 'network', 'Britz', 'Whilst', 'thinking', 'call', '16GB', '1', '3', 'arxiv.org', 'exposed', 'event', '12', 'Fast', 'faster', 'save', '26', 'author', 'parallelforall', 'investment', '2017', 'bugs.', 'rule', 'P100', '~hinton', 'perils-batch-norm.html', 'Lease', 'ab', 'while', 'offer', 'feed', 'exaggerate', 'Again', 'long', 'experience.', 'Quasi-Recurrent', 'practical', 'speed-ups', 'www.deeplearningbook.org', 'research-doc.credit-suisse.com', 'take', 'update', 'standard', '%', 's', 'worth', 'custom', 'Given', 'serialid=vGaaRmOdA9BYZyKVi', 'include', 'regardless', 'single', 'increase', 'gradient_checking', 'million', 'government', 'Precision', 'imgur.com', 'hCxOT', 'NMT', 'scary', 'dirty', 'direct', 'attention', 'fit', 'field', 'www.tensorflow.org', 'amis', 'indication', 'architecture', 'devblogs.nvidia.com', 'add', 'AWS', 'XLA', 'Background', 'understanding', '?', 'number', 'stuck', 'flow', 'concerning'}\n",
      "{'beyond', 'duck.co', 'actually', 'group', 'sent', 'people', 'exposure', 'as', 'love', 'man', 'serve', 'attribution', 'hit', 'archive', 'matter', 'DuckDuckHack', 'curious', 'unjust', 'Perl', 'program', 'story', 'for-profit', 'experience', 'need', 'sure', 'duckduckhack.com', 'Answers.', 'focus', 'i', 'While', 'amount', 'area', 'continually', 'Once', 'instant', 'could', 'feedback', 'That', 'draft', 'generally', 'blogged', 'periodically', 'policy', 'duckduckgo.com', 'api', 'Open', 'until', 'alternative', '/i', 'round', 'before', 'recruit', 'DM', 'inclusion', 'interview', 'appreciate', 'DuckDuckGo', 'track', 'APIs', 'branch', 'platform', 'notice', 'ready', 'anything', 'decent', 'team', 'open', 'MySQL', 'related', 'some', 'search.', 'Additionally', 'engine', 'Answers', 'craft', 'world', 'someone', 'incorporate', 'palm', 'search', 'freely', 'try', '--', 'goal', 'startup', 'Twitter', 'real', 'since', 'Remote', 'blogging', 'via', 'So', 'believe', 'fresh-eyes', 'developer', 'wherever', 'Greek', 'therefore', 'hire', 'www.youtube.com', 'English', 'React', 'family', 'ago', 'Instant', 'whole', 'etc', 'honestly', 'Also', 'maintain', 'traffic', 'Philly', 'meetup', 'particular', 'monetarily', 'benefit', 'improve', 'shudder', 'reference', 'spread', 'non', 'page', 'toggle', 'Andrew', 'ecosystem', 'improvement', 'unsure', 'tractionbook', 'local', 'HN.', 'access', 'company', 'private', 'API', 't.', 'fact', 'expand', 'underlie', 'region', 'create', 'everyone', 'privacy', 'Thanks', 'entire', 'face', 'themselves', 'develop', 'valid', 'expertise', 'answer'}\n"
     ]
    }
   ],
   "source": [
    "# Execute this cell.\n",
    "\n",
    "# Create sets of lemmas.\n",
    "merity_set = set(merity_lemmas)\n",
    "weinberg_set = set(weinberg_lemmas) \n",
    "\n",
    "# See which words are unique to each author.\n",
    "only_merity = merity_set - weinberg_set\n",
    "only_weinberg = weinberg_set - merity_set\n",
    "print(only_merity)\n",
    "print(only_weinberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can play around with the data in this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **7c** But more interesting are those words that are used by both authors. Use a similar set operation to see how many words are commonly used by both authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are  167 tokens are commonly used by both authors.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "both = merity_set & weinberg_set\n",
    "print (\"Here are \", len(both), \"tokens are commonly used by both authors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we find out which word is more typical for which author? We could easily implement log likelihood function based on [this page](http://ucrel.lancs.ac.uk/llwizard.html) by Paul Rayson. \n",
    "Log likelihood is a measure of 'unexpectedness' of a word in a particular corpus. We can compute it by comparing two corpora. We expect any given word to occur about the same amount of times in both corpora. If a word occurs in one corpus much more than in some other corpus, we get a high log likelihood score. We won't require this in the assignment, but if you have time, feel free to try and implement this too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Questions about Python\n",
    "\n",
    "* **8a** what is the difference between parameters and arguments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: An argument in context with functions is the actual value that is passed to the function ( as input) when it is called. However, parameter refers to the variables that are used in the function declaration/definition to represent those arguments that were sent to the function during the function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8b** can you give an example code that shows the local and global scope of a variable? Also add explanation as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local c: 18\n",
      "global c: 6\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def add(a,b):\n",
    "    c = a+b\n",
    "    print(\"local c:\",c)  # variable inside a function stay in the function, it is a local variable\n",
    "    return c\n",
    "\n",
    "a = 10\n",
    "b = 8\n",
    "c = 6\n",
    "add(a,b)\n",
    "print (\"global c:\",c)  # variable outside of the function is a global variable. Will not change by function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8c** What is a difference between CSV and TSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: CSV fields are separated by a comma \",\" . TSV fields are separated by a tabulation <TAB> or \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8d** When would you use JSON and when would you choose a table (CSV/TSV)? Give an example of both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  When my text have no comma in between, I can choose CSV. Such as a list of fruits or a table of numbers.\n",
    "\n",
    "When I have to include commas, I will use TSV. Such as text from an article.\n",
    "\n",
    "when I need a dictionary, I will use JSON. Such as a dictionary to store students'name and their grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8e** How many for-loops can you nest in one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As many as you want, but too many loops nest will lead to reading difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8f** Is it better to use file.close() or to use the contextmanager with the 'with' keyword? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It is better to use \"with\", because it is easier, and don't need to close anymore, which is easy to forget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **8g** What is the difference between the modes 'w' and 'a' when opening a file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer:\"w\" is writing mode, can lead to overwriting. \"a\" is append mode, will continue your file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

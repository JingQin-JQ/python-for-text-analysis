{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Off to analyzing text\n",
    "\n",
    "Way to go! You have already learnt a lot of essential components of the Python language. Being able to deal with data structures, import packages, build your own functions and operate with files is not only essential for most tasks in Python, but also a prerequisite for text analysis. We have applied some common preprocessing steps like casefolding/lowercasing, punctuation removal and stemming/lemmatization. Did you know that there are some very useful NLP packages and modules that do some of these steps? One that is often used in text analysis is the Python package **NLTK (the Natural Language Toolkit)**.\n",
    "\n",
    "### At the end of this chapter, you will be able to:\n",
    "* have an idea of the NLP tasks that constitute a pipeline\n",
    "* use the functions of the NLTK module to manipulate the content of files for NLP purposes (e.g. sentence splitting, tokenization, POS-tagging, and lemmatization);\n",
    "* do nesting of multiple for-loops or files\n",
    "\n",
    "### If you want to learn more about these topics, you might find the following links useful:\n",
    "* [SpaCy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text processing\n",
    "There are many aspects of text that one can interpret. One aspect is which part-of-speech does a certain word have (e.g. noun or verb). Another one is which entities (people, organizations, locations) can be found in text. A third one is: what is the dependency between the words and phrases in text. And so on. Each of these aspects is addressed within its own <span style=\"background-color:yellow\">NLP (Natural Language Processing) task</span>. \n",
    "\n",
    "Sometimes we put these tasks in a sequence, because they depend on each other. For instance, we need to first tokenize the text (split it into words) in order to be able to assign part-of-speech to each word. This sequence is often called an <span style=\"background-color:yellow\">NLP pipeline</span>. You can see the NLP pipeline of the NewsReader project [here](http://www.newsreader-project.eu/files/2014/02/SystemArchitecture.png) (you can ignore the middle part of the picture, and focus on the blue and green boxes in the outer row).\n",
    "\n",
    "In this chapter we will look into four simple NLP modules that are nevertheless very common in NLP: <span style=\"background-color:yellow\">tokenization, sentence splitting, lemmatization and POS tagging</span>. There are some more fun and advanced processing modules in the assignment of this block! ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The NLTK package\n",
    "\n",
    "Without further due, let's jump on to NLTK! To be able to use the modules that are part of the NLTK toolkit, we first need to *import* NLTK into our Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported NLTK, now we can perform the text processing we want. Here is a very simple example code that shows the complete processing with four modules: tokenization, sentence splitting, POS tagging, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This example sentence is used for illustrating some basic NLP tasks. Language is awesome!\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Sentence splitting\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lemma=lmtzr.lemmatize(tokens[4], 'v')\n",
    "\n",
    "# Printing all information\n",
    "print(tokens)\n",
    "print(sentences)\n",
    "print(tagged_tokens)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look into each of these processing modules in more detail ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization and sentence splitting with NLTK\n",
    "\n",
    "Amongst other things, the NLTK toolkit allows you to tokenize texts with the function `word_tokenize()`. To be able to use this function, we first need to download the NLTK Tokenizer Models. You can run `nltk.download(\"book\")` to download a collection of NLTK data and models, though we already have them, as downloaded from github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try tokenizing our Charlie story! First, we will open and read the file again and assign the file contents to the variable `content`. Then, we can call the `word_tokenize()` function from the `nltk` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Data/Charlie/charlie.txt\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(content)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have a list of all words in the text. The punctuation marks are also in the list, but as separate tokens. Another thing that NLTK can do for you is to split a text into sentences by using the `sent_tokenize()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(content)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now do all sorts of cool things with these lists. For example, we can search for all words that have certain letters in them and add them to a list. Let's say we want to find all present participles in the text. We know that present participles end with *-ing*, so we can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_participles = []\n",
    "for token in tokens:\n",
    "    if token.endswith(\"ing\"):\n",
    "        present_participles.append(token)\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good! We now have a list of words like *boiling*, *sizzling*, etc. But wait... Oops, there is one word in the list that actually is not a present participle! Of course, also other words can end with *-ing*. So if we want to find all present participles, we have to come up with a smarter solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. POS tagging\n",
    "Once again, NLTK comes to the rescue. Using the function `pos_tag()`, we can label each word in the text with its part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of tuples. The first element of the tuple is the token, the second element indicates the part of speech of the token. This POS tagger uses the POS tag set of the Penn Treebank Project, which can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). In this tag set, the `VBG` tag is used for present participles and gerunds. \n",
    "\n",
    "**Exercise:** Now let's try to make a list of all present participles in `charlie.txt` using the POS tags. Finish the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finish the following code:\n",
    "present_participles = []\n",
    "for token, pos in tagged_tokens:\n",
    "    if pos == \"VBG\":\n",
    "        # ??\n",
    "print(present_participles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(present_participles) == 11 and type(present_participles[0]) == str\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following list: ['boiling', 'bubbling', 'hissing', 'sizzling', 'clanking', 'running', 'hopping', 'knowing', 'rubbing', 'cackling', 'going']\n",
    "\n",
    "**Exercise:** Finish the following code to get *all* verbs. We already provided you with the full set of verb tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finish the following code:\n",
    "verb_tags = (\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\")\n",
    "verbs = []\n",
    "# Use a for-loop! \n",
    "\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(verbs) == 39 and type(verbs[0]) == str    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Lemmatization\n",
    "We now have a list of all inflected forms of the verbs. We can also use NLTK to lemmatize words. We will use the WordNetLemmatizer for this. In the code below, we loop through the list of verbs, lemmatize each of the verbs, and add them to a new list called `verb_lemmas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "verb_lemmas = []\n",
    "for participle in verbs:\n",
    "    # For this lemmatizer, we need to indicate the POS of the word (in this case, v = verb)\n",
    "    lemma = lmtzr.lemmatize(participle, \"v\") \n",
    "    verb_lemmas.append(lemma)\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We need to specify a POS tag to the WordNet lemmatizer, in a WordNet format (\"n\" for noun, \"v\" for verb, \"a\" for adjective). If we do not indicate the Part-of-Speech tag, the WordNet lemmatizer thinks it is a noun (this is the default value for its part-of-speech). See the examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nouns = ('building', 'applications', 'leafs')\n",
    "for n in test_nouns:\n",
    "    print(f\"Noun in conjugated form: {n}\")\n",
    "    default_lemma=lmtzr.lemmatize(n) # default lemmatization, without specifying POS, n is interpretted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma=lmtzr.lemmatize(n, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma=lmtzr.lemmatize(n, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_verbs=('grew', 'standing', 'plays')\n",
    "for v in test_verbs:\n",
    "    print(f\"Verb in conjugated form: {v}\")\n",
    "    default_lemma=lmtzr.lemmatize(v) # default lemmatization, without specifying POS, v is interpretted as a noun!\n",
    "    print(f\"Default lemmatization: {default_lemma}\")\n",
    "    verb_lemma=lmtzr.lemmatize(v, 'v')\n",
    "    print(f\"Lemmatization as a verb: {verb_lemma}\")\n",
    "    noun_lemma=lmtzr.lemmatize(v, 'n')\n",
    "    print(f\"Lemmatization as a noun: {noun_lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we observe here? The lemmatizer assumes every word is a noun unless specified diferently. We need to be careful and specify the POS tag because otherwise we will end up with wrong lemmatization such as the cases shown in the past two cells. For example, by default WordNet thinks that \"grew\" is a noun, and it will not lemmatize it as a past-tense verb.\n",
    "\n",
    "Luckily, in 2.2. we learned that we can also automatically infer the POS tags for each word. We can use these automatic POS tags as input to our lemmatizer to improve its accuracy for non-nouns. As an intermediate step, we need to translate the POS tags that we get from our POS tagger (this are according to the Penn TreeBank classification) to WordNet POS tags. Here is an example of how to lemmatize your words in a proper way, accounting for different POS tags (you can also read [this discussion](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing (the proper way, accounting for different POS tags)\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"\n",
    "    Returns the corresponding WordNet POS tag for a Penn TreeBank POS tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in ['RB', 'RBR', 'RBS']:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in ['JJ', 'JJR', 'JJS']:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lemmas = list()\n",
    "for token, pos in tagged_tokens:\n",
    "    wn_tag = penn_to_wn(pos) # convert Penn Treebank POS tag to WordNet POS tag\n",
    "    if not wn_tag == None:\n",
    "        lemma = lmtzr.lemmatize(token, wn_tag)\n",
    "    else:\n",
    "        lemma = lmtzr.lemmatize(token)\n",
    "    lemmas.append(lemma)\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The resulting list `verb_lemmas` above contains a lot of duplicates. Do you remember how you can get rid of these duplicates? Create a set in which each verb occurs only once and name it `unique_verbs`. Then print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(unique_verbs) == 28    \n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now use a for-loop to count the number of times that each of these verb lemmas occurs in the text! For each verb in the list you just created, get the count of this verb in `charlie.txt` using the `count()` method. Create a dictionary that contains the lemmas of the verbs as keys, and the counts of these verbs as values. Refer to the notebook about Topic 1 if you forgot how to use the `count()` method or how to create dictionary entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verb_counts = {}\n",
    "# Finish this for-loop\n",
    "for verb in unique_verbs:\n",
    "    # ??\n",
    "\n",
    "print(verb_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here! If your code is correct, you should get a compliment :-)\n",
    "assert len(verb_counts) == 28 and verb_counts[\"bubble\"] == 1 and verb_counts[\"be\"] == 9\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nesting\n",
    "\n",
    "So far we typically used a single for-loop or we were opening a single file at a time. In Python (and most programming languages), one can <span style=\"background-color:yellow\">nest</span> multiple loops or files in one another. For instance, we can use one (external) for-loop to iterate through files, and then for each file iterate through all its sentences (internal for-loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"): \n",
    "    with open(filename, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "        sentences = nltk.sent_tokenize(content)\n",
    "        print(\"INFO: File %s has %d sentences\" % (filename, len(sentences))) # We use %-notation and fstrings interchangeably to print strings.\n",
    "        counter=0\n",
    "        for sentence in sentences:\n",
    "            counter+=1\n",
    "            tokens=nltk.word_tokenize(sentence)\n",
    "            print(\"Sentence %d has %d tokens\" % (counter, len(tokens)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might think: can we stretch this on more levels? Iterate through files, then iterate through the sentences in these files, then iterate through each word in these sentences, then iterate through each letter in these words, etc. \n",
    "\n",
    "Yes, that is possible. Python (and most programming languages) allow you to perform nesting with (in theory) as many loops as you want. Keep in mind that nesting too much will eventually cause computational problems, but this depends also on the size of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we know how to process multiple files and how to write a function. Let's try to use these skills to get all nouns from Vickie's dream reports! Remember how we tagged all tokens with their POS tags on a single text file? We had to open the file, read the contents, tokenize the text, and use the POS tagger (remember, we first needed to import `nltk` to be able to use it). We can now write a single function that does all that for us. The following function reads the specified file and returns the tokens with their POS tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tag_tokens_file(filename):\n",
    "    \"\"\"Read the contents of FILENAME and return a list of its tokens with their POS tags.\"\"\"\n",
    "    with open(filename, \"r\") as infile:\n",
    "        content = infile.read()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of having to open a file, read the contents and close the file, we can just call the function `tag_tokens_file` to do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"../Data/Dreams/vickie1.txt\"\n",
    "tagged_tokens = tag_tokens_file(filename)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this for each of the files in the `../Data/Dreams` directory by using a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "    print(filename, \"\\n\", tagged_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extend this code a bit so that we don't print all POS-tagged tokens of each file, but we get all (proper) nouns from the texts and add them to a list called `nouns_in_dreams`. Then, we print the set of nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list that will contain all nouns\n",
    "nouns_in_dreams = []\n",
    "\n",
    "# Iterate over the `.txt` files in the directory and perform POS tagging on each of them\n",
    "for filename in glob.glob(\"../Data/Dreams/*.txt\"): \n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "        \n",
    "    # Get all (proper) nouns in the text (\"NN\" and \"NNP\") and add them to the list\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos in [\"NN\", \"NNP\"]:\n",
    "            nouns_in_dreams.append(token)\n",
    "\n",
    "# Print the set of nouns in all dreams\n",
    "print(set(nouns_in_dreams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an idea what Vickie dreams about!\n",
    "\n",
    "## 5. SpaCy\n",
    "[SpaCy](https://spacy.io/) is another module for text processing that is commonly used. In general, SpaCy often has better accuracy and especially speed than NLTK. Other than that, both of these modules are very rich with functionalities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
